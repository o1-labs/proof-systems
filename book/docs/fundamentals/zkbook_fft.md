---
title: Fast Fourier Transform (FFT)
---

### Fast Fourier Transform (FFT)

This section describes how the Cooley-Tukey fast Fourier transform works. As we
learned in the previous section, the key is to select evaluation points that
yield an efficient FFT algorithm.

Specifically, say we have $\omega \in F$ such that $\omega^{n} = 1$, and
$\omega^r \neq 1$ for any $0 < r < n$.

Put another way, all the values
$1, \omega, \omega^2, \omega^3, \dots, \omega^{n-1}$ are distinct and
$\omega^n = 1$.

Put yet another way, the group generated by $\omega$ inside $F^\times$ (written
$\langle \omega \rangle$) has size $n$.

We call such an $\omega$ a primitive $n$-th root of unity.

Suppose we have an $\omega$ which is a primitive $2^k$th root of unity and let
$A_k = \{ 1, \omega, \dots, \omega^{2^k - 1} \}$.

The FFT algorithm will let us compute $\mathsf{interp}_{A_k}$ for this set.

Actually, it is easier to see how it will let us compute the
$\mathsf{eval}_{A_k}$ algorithm efficiently.

We will describe an algorithm $\mathsf{FFT}(k, \omega, f)$ that takes as input

- $k \in \N$
- $\omega \in F$ a primitive $2^k$th root of unity
- $f \in F[x]_{< 2^k}$ in dense coefficients form (i.e., as a vector of
  coefficients of length $n$).

and outputs the vector of evaluations

$$
[f(1), f(\omega), f(\omega^2) \dots, f(\omega^{2^k - 1})]
$$

and does it in time $O(k 2^k)$ (which is to say, $n \log n$ if $n = 2^k$).

Notice that naively, computing each evaluation $f(\omega^i)$ using the
coefficients of $f$ would require time $O(n)$, and so computing all $n$ of them
would require time $O(n^2)$.

The algorithm $\mathsf{FFT}(k, \omega, f)$ can be defined recursively as
follows.

If $k = 0$, then $\omega$ is a primitive $1$st root of unity, and $f$ is a
polynomial of degree $0$. That means $\omega = 1$ and also $f$ is a constant
$c \in F$. So, we can immediately output the array of evaluations
$[c] = [f(1)]$.

If $k > 0$, then we will split $f$ into two polynomials, recursively call
$\mathsf{FFT}$ on them, and reconstruct the result from the recursive calls.

To that end, define $f_0$ to be the polynomial whose coefficients are all the
even-index coefficients of $f$ and $f_1$ the polynomial whose coefficients are
all the odd-index coefficients of $f$. In terms of the array representation,
this just means splitting out every other entry into two arrays. So that can be
done in time $O(n)$.

Write $f = \sum_{i < 2^k} c_i x^i$, so that
$f_0 = \sum_{i < 2^{k - 1}} c_{2i} x^i$ and
$f_1 = \sum_{i < 2^{k-1}} c_{2i + 1} x^i$. Then

$$
\begin{aligned}
f(x)
&= \sum_{i < 2^k} c_i x^i \\
&= \sum_{i < 2^{k-1}} c_{2i} x^{2i} + \sum_{i < 2^{k-1}} c_{2i + 1} x^{2i + 1} \\
&= \sum_{i < 2^{k-1}} c_{2i} (x^2)^i+ \sum_{i < 2^{k-1}} c_{2i + 1} x \cdot (x^2)^i  \\
&= \sum_{i < 2^{k-1}} c_{2i} (x^2)^i+ x \sum_{i < 2^{k-1}} c_{2i + 1} (x^2)^i  \\
&= f_0(x^2) + x f_1(x^2)
\end{aligned}
$$

Now, notice that if $\omega$ is a $2^k$th root of unity, then $\omega^2$ is a
$2^{k - 1}$th root of unity. Thus we can recurse with
$\mathsf{FFT}(k - 1, \omega^2, f_0)$ and similarly for $f_1$. Let

$$
\begin{aligned}
[e_{0, 0}, \dots, e_{0, 2^{k-1} - 1}] &= \mathsf{FFT}(k-1, \omega^2, f_0) \\
[e_{1, 0}, \dots, e_{1, 2^{k-1} - 1}] &= \mathsf{FFT}(k-1, \omega^2, f_1)
\end{aligned}
$$

By assumption $e_{i, j} = f_i((\omega^2)^j)$. So, for any $j$ we have

$$
\begin{aligned}
f(\omega^j)
&= f_0((\omega^2)^j) + \omega^j f_1((\omega^2)^j)
\end{aligned}
$$

Now, since $j$ may be larger than $2^{k-1} - 1$, we need to reduce it mod
$2^{k-1}$, relying on the fact that if $\tau$ is an $n$th root of unity then
$\tau^j = \tau^{j \mod n}$ since $\tau^n = 1$. Thus,
$(\omega^2)^j = (\omega^2)^{j \mod 2^{k-1}}$ and so we have

$$
\begin{aligned}
f(\omega^j)
&= f_0((\omega^2)^{j \mod 2^{k-1}} ) + \omega^j f_1((\omega^2)^{j \mod 2^{k-1}}) \\
&= e_{0, j \mod 2^{k-1}} + \omega^j e_{1, j \mod 2^{k-1}}
\end{aligned}
$$

We can compute the array $W = [ 1, \omega, \dots, \omega^{2^k - 1}]$ in time
$O(n)$ (since each entry is the previous entry times $\omega$). Then we can
compute each entry of the output in $O(1)$ as

$$
\begin{aligned}
f(\omega^j)
&= e_{0, j \mod 2^{k-1}} + W[j] \cdot e_{1, j \mod 2^{k-1}}
\end{aligned}
$$

There are $n$ such entries, so this takes time $O(n)$.

This concludes the recursive definition of the algorithm
$\mathsf{FFT}(k, \omega, f)$.

> **Algorithm: computing $\mathsf{eval}_{A_k}$**
>
> - $\mathsf{Input~} f = [c_0, \ldots, c_{2^k - 1}]$ the coefficients of
>   polynomial $f(x) = \sum_{i < 2^k} c_i x^i$
> - $\mathsf{Compute~} W \gets \left[1, \omega, \omega^2, ..., \omega^{2^k - 1}\right]$
> - $\mathsf{FFT}(k, \omega, f) \rightarrow \left[f(1), f(\omega), f(\omega^2) \dots, f(\omega^{2^k - 1})\right]$
>   - $\mathtt{if~} k == 0$
>     - $\mathtt{return~} f$
>   - $\mathtt{else}$
>     - $\mathsf{Compute~} f_0 = [c_0, c_2, ..., c_{2^k - 2}]$ the even
>       coefficients of $f,$ corresponding to
>       $f_0(x) = \sum_{i < 2^{k - 1}} c_{2i} x^i$
>     - $\mathsf{Compute~} f_1 = [c_1, c_3, ..., c_{2^k - 1}]$ the odd
>       coefficients of $f,$ corresponding to
>       $f_1(x) = \sum_{i < 2^{k - 1}} c_{2i + 1} x^i$
>     - $e_0 \gets \mathsf{FFT}(k - 1, \omega^2, f_0)$
>     - $e_1 \gets \mathsf{FFT}(k - 1, \omega^2, f_1)$
>     - $\mathtt{for~} j \in [0, 2^k - 1]$
>       - $F_j \gets e_{0, j \mod 2^{k - 1}} + W[j] \cdot e_{1, j \mod 2^{k - 1}}$
>     - $\mathtt{return~} F$

Now let's analyze the time complexity. Let $T(n)$ be the complexity on an
instance of size $n$ (that is, for $n = 2^k$).

Looking back at what we have done, we have done

- $O(n)$ for computing $f_0$ and $f_1$
- two recursive calls, each of size $n / 2$
- $O(n)$ for computing the powers of $\omega$
- $O(n)$ for combining the results of the recursive calls

In total, this is $O(n) + 2 T(n / 2)$. Solving this recurrence yields
$T(n) = O(n) \cdot log n = O(n \log n)$. Basically, there are $\log n$
recursions before we hit the base case, and each step takes time $O(n)$.
$\square$

Now, in practice there are ways to describe this algorithm non-recursively that
have better concrete performance, but that's out of scope for this document.
Read the code if you are interested.

#### Using the FFT algorithm to compute $\mathsf{interp}_{A_k}$

So far we have a fast way to compute $\mathsf{eval}_{A_k}(f)$ all at once, where
$A_k$ is the set of powers of a $2^k$th root of unity $\omega$. For convenience
let $n = 2^k$.

Now we want to go the other way and compute a polynomial given an array of
evaluations. Specifically, $n$ evaluations
$\left[f(x_0), f(x_1), ..., f(x_{n - 1})\right]$ uniquely define a degree
$n - 1$ polynomial. This can be written as a system of $n$ equations

$$
\begin{aligned}
f(x_0) &= c_0 + c_1x_0 + \ldots + c_{n - 1}x_0^{n - 1} \\
f(x_1) &= c_0 + c_1x_1 + \ldots + c_{n - 1}x_1^{n - 1} \\
\vdots\\
f(x_{n - 1}) &= c_0 + c_1x_{n - 1} + \ldots + c_{n - 1}x_{n - 1}^{n - 1}, \\
\end{aligned}
$$

which can be rewritten as a matrix vector product.

$$
\begin{bmatrix}
    f(x_0) \\
    f(x_1) \\
    \vdots \\
    f(x_{n - 1})
\end{bmatrix}
=
\begin{bmatrix}
    1 & x_0 & \cdots & x_0^{n - 1} \\
    1 & x_1 & \cdots & x_1^{n - 1} \\
    \vdots & \vdots & \ddots & \vdots\\
    1 & x_{n - 1} & \cdots & x_{n - 1}^{n - 1} \\
\end{bmatrix}
\times
\begin{bmatrix}
    c_{0} \\
    c_{1} \\
    \vdots \\
    c_{n - 1}
\end{bmatrix}
$$

This $n \times n$ matrix is a Vandermonde matrix and it just so happens that
square Vandermonde matrices are invertible, iff the $x_i$ are unique. Since we
purposely selected our $x_i$ to be the powers of $\omega$, a primitive $n$-th
root of unity, by definition $x_i = \omega^i$ are unique.

Therefore, to compute the polynomial given the corresponding array of
evaluations (i.e. interpolation) we can solve for the polynomial's coefficients
using the inverse of the matrix.

$$
\begin{bmatrix}
    c_{0} \\
    c_{1} \\
    \vdots \\
    c_{n - 1}
\end{bmatrix}
=
\begin{bmatrix}
    1 & 1 & \cdots & 1^{n - 1} \\
    1 & \omega & \cdots & \omega^{n - 1} \\
    \vdots & \vdots & \ddots & \vdots\\
    1 & \omega^{n - 1} & \cdots & \omega^{(n - 1)(n - 1)} \\
\end{bmatrix}^{-1}
\times
\begin{bmatrix}
    f(1) \\
    f(\omega) \\
    \vdots \\
    f(\omega^{n - 1})
\end{bmatrix}
$$

All we need now is the inverse of this matrix, which is slightly complicated to
compute. I'm going to skip it for now, but if you have the details please make a
pull request.

Substituting in the inverse matrix we obtain the equation for interpolation.

$$
\begin{bmatrix}
    c_{0} \\
    c_{1} \\
    \vdots \\
    c_{n - 1}
\end{bmatrix}
=
\frac{1}{n}
\begin{bmatrix}
    1 & 1 & \cdots & 1^{n - 1} \\
    1 & \omega^{-1} & \cdots & \omega^{-(n - 1)} \\
    \vdots & \vdots & \ddots & \vdots\\
    1 & \omega^{-(n - 1)} & \cdots & \omega^{-(n - 1)(n - 1)} \\
\end{bmatrix}
\times
\begin{bmatrix}
    f(1) \\
    f(\omega) \\
    \vdots \\
    f(\omega^{n - 1})
\end{bmatrix}
$$

Observe that this equation is nearly identical to the original equation for
evaluation, except with the following substitution.

$$
\omega^i \Rightarrow \frac{1}{n}\omega^{-1i}
$$

Consequently and perhaps surprisingly, we can reuse the FFT algorithm
$\mathsf{eval}_{A_k}$ in order to compute the inverse-- $\mathsf{interp}_{A_k}$.

So, suppose we have an array $[a_0, \dots, a_{n-1}]$ of field elements (which
you can think of as a function $A_k \to F$) and we want to compute the
coefficients of a polynomial $f$ with $f(\omega^i) = a_i$.

To this end, define a polynomial $g$ by $g = \sum_{j < n} a_j x^j$. That is, the
polynomial whose coefficients are the evaluations in our array that we're hoping
to interpolate.

Now, let $[e_0, \dots, e_{n-1}] = \mathsf{FFT}(k, \omega^{-1}, g)$.

That is, we're going to feed $g$ into the FFT algorithm defined above with
$\omega^{-1}$ as the $2^k$th root of unity. It is not hard to check that if
$\omega$ is an n-th root of unity, so is $\omega^{-1}$. Remember: the resulting
values are the evaluations of $g$ on the powers of $\omega^{-1}$, so
$e_i = g(\omega^{-i}) = \sum_{j < n} a_j \omega^{-ij}$.

Now, let $h = \sum_{i < n} e_i x^i$. That is, re-interpret the values $e_i$
returned by the FFT as the coefficients of a polynomial. I claim that $h$ is
almost the polynomial we are looking for. Let's calculate what values $h$ takes
on at the powers of $\omega$.

$$
\begin{aligned}
h(\omega^s)
&= \sum_{i < n } e_i \omega^{si} \\
&= \sum_{i < n} \omega^{si} \sum_{j < n} a_j \omega^{-ij} \\
&= \sum_{i < n} \sum_{j < n} a_j \omega^{si-ij} \\
&= \sum_{j < n} a_j \sum_{i < n} \omega^{i(s-j)} \\
\end{aligned}
$$

Now, let's examine the quantity $c_j := \sum_{i < n} \omega^{i(s-j)}$. We claim
that if $j = s$, then $c_j = n$, and if $j \neq s$, then $c_j = 0$. The first
claim is clear since

$$
c_s = \sum_{i<n} \omega^{i(s-s)} = \sum_{i < n} \omega^0 = \sum_{i<n} 1 = n
$$

For the second claim, we will prove that $\omega^{s-j} c_j = c_j$. This implies
that $(1 - \omega^{s-j}) c_j = 0$. So either $1 - \omega^{s-j} = 0$ or
$c_j = 0$. The former cannot be the case since it implies $\omega^{s-j} =1$
which in turn implies $s = j$ which is impossible since we are in the case
$j \neq s$. Thus we have $c_j = 0$ as desired.

So let's show that $c_j$ is invariant under multiplication by $\omega^{s-j}$.
Basically, it will come down to the fact that $\omega^n = \omega^0$.

$$
\begin{aligned}
\omega^{s-j} c_j
&= \omega^{s-j} \sum_{i<n} \omega^{i(s-j)} \\
&= \sum_{i < n} \omega^{i(s-j) + (s-j)} \\
&= \sum_{i < n} (\omega^{i+1})^{s-j} \\
&= (\omega^{0+1})^{s-j} + (\omega^{1+1})^{s-j} + \dots + (\omega^{(n-1)+1})^{s-j} \\
&= (\omega^{1})^{s-j} + (\omega^{2})^{s-j} + \dots + (\omega^{n})^{s-j} \\
&= (\omega^{1})^{s-j} + (\omega^{2})^{s-j} + \dots + (\omega^{0})^{s-j} \\
&= (\omega^{0})^{s-j} + (\omega^{1})^{s-j} + \dots + (\omega^{n-1})^{s-j} \\
&= \sum_{i < n} (\omega^i)^{s-j} \\
&= c_j
\end{aligned}
$$

So now we know that

$$
\begin{aligned}
h(\omega^s)
&= \sum_{j < n} a_j c_j \\
&= a_s \cdot n + \sum_{j \neq s} a_j \cdot 0 \\
&= a_s \cdot n
\end{aligned}
$$

So if we define $f = h / n$, then $f(\omega^s) = a_s$ for every $s$ as desired.
Thus we have our interpolation algorithm, sometimes called an inverse FFT or
IFFT:

> **Algorithm: computing $\mathsf{interp}_{A_k}$**
>
> 0. Input: $[a_0, \dots, a_{n-1}]$ the points we want to interpolate and
>    $\omega$ a $n$th root of unity.
> 1. Interpret the input array as the coefficients of a polynomial
>    $g = \sum_{i < n} a_i x^n$.
> 2. Let $[e_0, \dots, e_n] = \mathsf{FFT}(k, \omega^{-1}, g)$.
> 3. Output the polynomial $\sum_{i < n}(e_i / n) x^i$. I.e., in terms of the
>    dense-coefficients form, output the vector $[e_0 / n, \dots, e_{n - 1}/n]$.

Note that this algorithm also takes time $O(n \log n)$

## Takeaways

- Polynomials can be represented as a list of coefficients or a list of
  evaluations on a set $A$

- If the set $A$ is the set of powers of a root of unity, there are time
  $O(n \log n)$ algorithms for converting back and forth between those two
  representations

- In evaluations form, polynomials can be added and multiplied in time $O(n)$
  - TODO: caveat about hitting degree

### Exercises

- Implement types `DensePolynomial<F: FfftField>` and `Evaluations<F: FftField>`
  that wrap a `Vec<F>` and implement the FFT algorithms described above for
  converting between them

- Familiarize yourself with the types and functions provided by `ark_poly`
